{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip -q install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit                                                                     # Decorator that turns a Python function into a GPU kernel\n",
    "def add_kernel(x_ptr, y_ptr, out_ptr,                                           # Pointers to global memory\n",
    "               N,                                                               # Total number of elements to add\n",
    "               BLOCK_SIZE: tl.constexpr):                                       # tl.constexpr marks a parameter as compile-time constant\n",
    "\n",
    "                                                                                # A program instance is one independent unit of parallel execution in Triton\n",
    "    pid = tl.program_id(axis=0)                                                 # pid ∈ {0, 1, …, G−1} where G = ceil(N/BLOCK_SIZE)\n",
    "    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)                          # [pid * BLOCK_SIZE + 0, ..., pid * BLOCK_SIZE + BLOCK_SIZE-1]\n",
    "    mask = offs < N                                                             # [True, True, ..., False]\n",
    "    x = tl.load(x_ptr + offs, mask=mask)                                        # Read from global memory into registers\n",
    "    y = tl.load(y_ptr + offs, mask=mask)                                        # Read from global memory into registers\n",
    "    z = x + y                                                                   # Compute in registers\n",
    "    tl.store(out_ptr + offs, z, mask=mask)                                      # Write from registers to global memory\n",
    "\n",
    "\n",
    "\n",
    "def add_triton(x: torch.Tensor,                                                 # Python wrapper that defines the launch grid and calls the kernel\n",
    "               y: torch.Tensor, \n",
    "               block_size: int = 1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x, y: 1D or arbitrary-shaped CUDA tensors of same shape & dtype.\n",
    "    This wrapper flattens them, launches the kernel, and reshapes back.\n",
    "    \"\"\"\n",
    "    assert x.is_cuda and y.is_cuda, \"Use CUDA tensors\"\n",
    "    assert x.shape == y.shape and x.dtype == y.dtype\n",
    "\n",
    "    N = x.numel()\n",
    "    out = torch.empty_like(x)\n",
    "                                                                                # Triton needs a grid (how many program instances to launch).\n",
    "                                                                                # We want one program instance per BLOCK_SIZE chunk of elements.\n",
    "    grid = lambda meta: (triton.cdiv(N, meta[\"BLOCK_SIZE\"]),)                   # meta is a small dict of compile-time constants Triton supplies to the grid callable\n",
    "    add_kernel[grid](x.reshape(-1), y.reshape(-1), out.reshape(-1),             # We pass the grid callable to the kernel\n",
    "                     N,                                                         # Launch syntax: kernel[grid](args..., CONST1=..., CONST2=...)\n",
    "                     BLOCK_SIZE=block_size)                                     # x.reshape(-1), y.reshape(-1), out.reshape(-1): Pass flattened CUDA tensors as pointer arguments to the kernel.\n",
    "                                                                                # Strictly speaking, x.reshape(-1) is not itself a pointer.\n",
    "                                                                                # Triton automatically extracts the tensor’s underlying device pointer (the address of its data in GPU memory) when launching.\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1_000_000, device=\"cuda\", dtype=torch.float32)\n",
    "y = torch.randn_like(x)\n",
    "\n",
    "out = add_triton(x, y, block_size=1024)\n",
    "\n",
    "print(torch.allclose(out, x + y))\n",
    "\n",
    "import time\n",
    "def bench(fn, *args, warmup=10, iters=50):\n",
    "    for _ in range(warmup):\n",
    "        _ = fn(*args); torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = fn(*args); torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "print(\"Triton ms:\", 1e3 * bench(add_triton, x, y))\n",
    "print(\"PyTorch ms:\", 1e3 * bench(lambda a,b: a+b, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
