{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip -q install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit                                                                     # Decorator that turns a Python function into a GPU kernel\n",
    "def add_kernel(x_ptr, y_ptr, out_ptr,                                           # Pointers to global memory\n",
    "               N,                                                               # Total number of elements to add\n",
    "               BLOCK_SIZE: tl.constexpr):                                       # tl.constexpr marks a parameter as compile-time constant\n",
    "\n",
    "                                                                                # A program instance is one independent unit of parallel execution in Triton\n",
    "    pid = tl.program_id(axis=0)                                                 # pid ∈ {0, 1, …, G−1} where G = ceil(N/BLOCK_SIZE)\n",
    "    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)                          # [pid * BLOCK_SIZE + 0, ..., pid * BLOCK_SIZE + BLOCK_SIZE-1]\n",
    "    mask = offs < N                                                             # [True, True, ..., False]\n",
    "    x = tl.load(x_ptr + offs, mask=mask)                                        # Read from global memory into registers\n",
    "    y = tl.load(y_ptr + offs, mask=mask)                                        # Read from global memory into registers\n",
    "    z = x + y                                                                   # Compute in registers\n",
    "    tl.store(out_ptr + offs, z, mask=mask)                                      # Write from registers to global memory\n",
    "\n",
    "\n",
    "\n",
    "def add_triton(x: torch.Tensor,                                                 # Python wrapper that defines the launch grid and calls the kernel\n",
    "               y: torch.Tensor, \n",
    "               block_size: int = 1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x, y: 1D or arbitrary-shaped CUDA tensors of same shape & dtype.\n",
    "    This wrapper flattens them, launches the kernel, and reshapes back.\n",
    "    \"\"\"\n",
    "    assert x.is_cuda and y.is_cuda, \"Use CUDA tensors\"\n",
    "    assert x.shape == y.shape and x.dtype == y.dtype\n",
    "\n",
    "    N = x.numel()\n",
    "    out = torch.empty_like(x)\n",
    "                                                                                # Triton needs a grid (how many program instances to launch).\n",
    "                                                                                # We want one program instance per BLOCK_SIZE chunk of elements.\n",
    "    grid = lambda meta: (triton.cdiv(N, meta[\"BLOCK_SIZE\"]),)                   # meta is a small dict of compile-time constants Triton supplies to the grid callable\n",
    "    add_kernel[grid](x.reshape(-1), y.reshape(-1), out.reshape(-1),             # We pass the grid callable to the kernel\n",
    "                     N,                                                         # Launch syntax: kernel[grid](args..., CONST1=..., CONST2=...)\n",
    "                     BLOCK_SIZE=block_size)                                     # x.reshape(-1), y.reshape(-1), out.reshape(-1): Pass flattened CUDA tensors as pointer arguments to the kernel.\n",
    "                                                                                # Strictly speaking, x.reshape(-1) is not itself a pointer.\n",
    "                                                                                # Triton automatically extracts the tensor’s underlying device pointer (the address of its data in GPU memory) when launching.\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1_000_000, device=\"cuda\", dtype=torch.float32)\n",
    "y = torch.randn_like(x)\n",
    "\n",
    "out = add_triton(x, y, block_size=1024)\n",
    "\n",
    "print(torch.allclose(out, x + y))\n",
    "\n",
    "import time\n",
    "def bench(fn, *args, warmup=10, iters=50):\n",
    "    for _ in range(warmup):\n",
    "        _ = fn(*args); torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = fn(*args); torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "print(\"Triton ms:\", 1e3 * bench(add_triton, x, y))\n",
    "print(\"PyTorch ms:\", 1e3 * bench(lambda a,b: a+b, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit                                                                     # Decorator that turns a Python function into a GPU kernel\n",
    "def matmul_kernel(A_ptr, B_ptr, C_ptr,                                          # Pointers to global memory\n",
    "                  M, N, K,                                                      # Shapes: A → (M, K) B → (K, N) C → (M, N)\n",
    "                  stride_am, stride_ak,                                         # Stride allows for accessing elements of the matrices\n",
    "                  stride_bk, stride_bn,                                         # address = base + i * stride_row + j * stride_col\n",
    "                  stride_cm, stride_cn,\n",
    "                  BLOCK_M: tl.constexpr,                                        # Compile-time constants telling Triton the tile sizes along M, N, K\n",
    "                  BLOCK_N: tl.constexpr, \n",
    "                  BLOCK_K: tl.constexpr):\n",
    "  \n",
    "    pid_m = tl.program_id(0)                                                    # Each tile (block) of the output matrix is handled by one program instance\n",
    "    pid_n = tl.program_id(1)                                                    # pid_m and pid_n identify the tile position in the output matrix C (row-tile, column-tile)\n",
    "\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)                            # Offsets give the row (offs_m) and column (offs_n) indices of elements - \n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)                            # - inside that tile of the output matrix.\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_K)                                              # offs_k indexes the shared K dimension, i.e. the columns of A and the rows of B\n",
    "\n",
    "\n",
    "                                                                                # C = A x B  =>  C_ij = A_ik x B_kj\n",
    "                                                                                # When we tile:\n",
    "                                                                                # For A, each tile covers some rows of A (so moves down with pid_m) and all needed columns K.\n",
    "                                                                                # For B, each tile covers some columns of B (so moves right with pid_n) and all needed rows K.\n",
    "                                                                                # A’s tile start = (pid_m * BLOCK_M, 0) → move down for different row-blocks.\n",
    "                                                                                # B’s tile start = (0, pid_n * BLOCK_N) → move right for different column-blocks.\n",
    "\n",
    "\n",
    "\n",
    "    A_block_ptr = tl.make_block_ptr(A_ptr,                                      # A_ptr → base address of A.\n",
    "                                    (M, K),                                     # (M, K) → full matrix shape.\n",
    "                                    (stride_am, stride_ak),                     # (stride_am, stride_ak) → row/column strides\n",
    "                                    (pid_m * BLOCK_M, 0),                       # (pid_m * BLOCK_M, 0) → starting offset of this tile (top-left corner).\n",
    "                                    (BLOCK_M, BLOCK_K),                         # (BLOCK_M, BLOCK_K) → tile size this instance will load.\n",
    "                                    (1, 0))                                     # (1, 0) → memory traversal order (row-major).\n",
    "    \n",
    "    B_block_ptr = tl.make_block_ptr(B_ptr,                                      # B_ptr → base address of B.\n",
    "                                    (K, N),                                     # (K, N) → full matrix shape.\n",
    "                                    (stride_bk, stride_bn),                     # stride_bk, stride_bn) → row/column strides\n",
    "                                    (0, pid_n * BLOCK_N),                       # (0, pid_n * BLOCK_N)\n",
    "                                    (BLOCK_K, BLOCK_N),                         # (BLOCK_K, BLOCK_N) → tile size this instance will load.\n",
    "                                    (1, 0))                                     # (1, 0) → row-major traversal.\n",
    "    \n",
    "    acc = tl.zeros([BLOCK_M, BLOCK_N], tl.float32)                              # Creates a local tile of zeros in SRAM — shape (BLOCK_M, BLOCK_N) \n",
    "                                                                                # — to accumulate partial sums for the output tile of matrix C.\n",
    "\n",
    "    for k in range(0, K, BLOCK_K):                                              # Iterate over the K dimension in tiles of size BLOCK_K\n",
    "        A = tl.load(A_block_ptr)                                                # Load current A tile (BLOCK_M × BLOCK_K) from global → registers/SRAM.\n",
    "        B = tl.load(B_block_ptr)                                                # Load current B tile (BLOCK_K × BLOCK_N) from global → registers/SRAM.\n",
    "        acc += tl.dot(A, B)                                                     # Tile matmul and accumulate into the output tile (BLOCK_M × BLOCK_N)\n",
    "        A_block_ptr = tl.advance(A_block_ptr, (0, BLOCK_K))                     # Slide A’s block pointer right by BLOCK_K (next K-slice)\n",
    "        B_block_ptr = tl.advance(B_block_ptr, (BLOCK_K, 0))                     # Slide B’s block pointer down by BLOCK_K (next K-slice).\n",
    "\n",
    "    tl.store(C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn, # Computes the global addresses for the C tile and writes acc there\n",
    "             acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_triton(A: torch.Tensor, B: torch.Tensor,\n",
    "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32) -> torch.Tensor:\n",
    "\n",
    "    assert A.is_cuda and B.is_cuda, \"Use CUDA tensors\"\n",
    "    assert A.dtype == B.dtype, \"Dtypes must match\"\n",
    "    assert A.shape[-1] == B.shape[-2], \"Inner dims must match (A: MxK, B: KxN)\"\n",
    "\n",
    "    A_ = A.contiguous()   \n",
    "    B_ = B.contiguous()\n",
    "\n",
    "    M, K = A_.shape\n",
    "    K2, N = B_.shape\n",
    "    assert K == K2, \"Inner dims must match (A: MxK, B: KxN)\"\n",
    "\n",
    "    C = torch.empty((M, N), device=A.device, dtype=torch.float32) \n",
    "\n",
    "    stride_am, stride_ak = A_.stride()\n",
    "    stride_bk, stride_bn = B_.stride()\n",
    "    stride_cm, stride_cn = C.stride()\n",
    "\n",
    "    \n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))                   # Triton launch grid: (#tiles along M, #tiles along N)\n",
    "\n",
    "\n",
    "    matmul_kernel[grid](                                                        # ← Triton: kernel[grid](...)\n",
    "        A_, B_, C,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K                       # ← tl.constexpr params\n",
    "    )\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, warmup=5, iters=30, sync_cuda=False):\n",
    "    for _ in range(warmup):\n",
    "        _ = fn(*args)\n",
    "        if sync_cuda: torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = fn(*args)\n",
    "        if sync_cuda: torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "# sizes\n",
    "M, K, N = 1024, 1536, 768\n",
    "\n",
    "A_cpu = torch.randn(M, K, dtype=torch.float32)\n",
    "B_cpu = torch.randn(K, N, dtype=torch.float32)\n",
    "\n",
    "A_gpu = A_cpu.to(\"cuda\", dtype=torch.float16)\n",
    "B_gpu = B_cpu.to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "C_ref = A_cpu @ B_cpu\n",
    "C_tri = matmul_triton(A_gpu, B_gpu)             \n",
    "print(\"allclose (tri vs ref):\", torch.allclose(C_tri.cpu(), C_ref, atol=1e-2, rtol=0))\n",
    "\n",
    "t_tri = bench(matmul_triton, A_gpu, B_gpu, sync_cuda=True)\n",
    "print(f\"Triton GPU: {1e3*t_tri:.2f} ms\")\n",
    "\n",
    "def torch_cuda_mm(A,B): return (A.float() @ B.float())  \n",
    "t_torch = bench(torch_cuda_mm, A_gpu, B_gpu, sync_cuda=True)\n",
    "print(f\"Torch CUDA: {1e3*t_torch:.2f} ms\")\n",
    "\n",
    "# CPU baseline (single-threaded vs multithreaded may vary)\n",
    "t_cpu = bench(lambda a,b: a @ b, A_cpu, B_cpu, sync_cuda=False)\n",
    "print(f\"Torch CPU : {1e3*t_cpu:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
